{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes for \"Late 20th and 21st century lake energy balance response to warming in the tropical high Andes (revision for GPC)\"\n",
    "\n",
    "#### Jarunetr Sae-Lim, Bronwen Konecky, Carrie Morrill, Neal Michelutti, Christopher Grooms, and John Smol\n",
    "\n",
    "### Abstract\n",
    "The response of Andean high-alpine lakes (>4,000 m above sea level) to atmospheric warming is poorly understood, in part due to a lack of long-term limnological and meteorological observations. Here, we use in situ, reanalysis, and climate modeling output data paired with a one-dimensional lake energy balance model to investigate the response of lake thermal properties to observed and projected 20th and 21st century warming in the tropical high Andes of Peru. The lake model configuration is based on Lake Sibinacocha (13.86°S, 71.02°W, 4,860 m a.s.l.), the largest high-alpine lake in the Andes. Relationships between recent air and lake temperature changes were investigated using the model forced with 20th/21st-century ERA5-Land climate reanalysis data, and future relationships were investigated using two CMIP6 future climate scenarios with CESM2 (SSP2-4.5 and SSP5-8.5). Results show that Sibinacocha whole-lake temperature has warmed at a rate of 0.085°C/decade between 1981-2019, which is slower than other published estimates from lakes globally despite its high alpine location. Lake Sibinacocha temperatures display interannual variability that aligns with air temperature variations, suggesting that broad climatic teleconnections that affect Andean air temperatures also influence lake temperature and stratification. Under the SSP2-4.5 and SSP5-8.5 scenarios, the model indicates an acceleration of Lake Sibinacocha’s whole-lake warming rate by 4.2 to 8.6 times relative to the modern rate. By 2091-2100, Lake Sibinacocha is projected to increase 2.7°C to 6.1 °C. Lake Sibinacocha is projected to remain well-mixed throughout the 21st century which contributes to warming at all depths rather than increased stratification. Lake Sibinacocha is anticipated to respond more slowly to warming simply due to its size. Therefore, our results should be considered a conservative end-member for other lakes in the tropical high Andes, which due to their shallower sizes will likely respond more quickly to atmospheric warming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data\n",
    "We use ERA5-Land reanalysis for Part 1 (calibration) and CMIP6 CESM2 SSP245, SSP585, and historical ensembles for Part 2 (simulation). All inputs were bias corrected relative to observations from nearby weather stations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ERA5-Land Reanalysis (1981-2019)\n",
    "We retrieved the ERA5-Land data using ERA API through cdsapi package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "# https://anaconda.org/conda-forge/cdsapi\n",
    "# conda install -c conda-forge cdsapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cdsapi.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.retrieve(\n",
    "    'reanalysis-era5-land', #dataset you want\n",
    "    {\n",
    "        'format': 'netcdf',\n",
    "        'variable': [\n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind', '2m_dewpoint_temperature',\n",
    "            '2m_temperature', 'surface_pressure', #'runoff', \n",
    "            'surface_solar_radiation_downwards', 'surface_thermal_radiation_downwards', #'total_precipitation',\n",
    "        ], \n",
    "        'year': [\n",
    "            '1981', '1982', '1983',\n",
    "            '1984', '1985', '1986',\n",
    "            '1987', '1988', '1989',\n",
    "            '1990', '1991', '1992',\n",
    "            '1993', '1994', '1995',\n",
    "            '1996', '1997', '1998',\n",
    "            '1999', '2000', '2001',\n",
    "            '2002', '2003', '2004',\n",
    "            '2005', '2006', '2007',\n",
    "            '2008', '2009', '2010',\n",
    "            '2011', '2012', '2013',\n",
    "            '2014', '2015', '2016',\n",
    "            '2017', '2018', '2019',\n",
    "            '2020',\n",
    "            \n",
    "        ],\n",
    "        'month': [\n",
    "            '01', '02', '03',\n",
    "            '04', '05', '06',\n",
    "            '07', '08', '09',\n",
    "            '10', '11', '12',\n",
    "        ],\n",
    "        'day':[\n",
    "            '01','02','03',\n",
    "            '04','05','06',\n",
    "            '07','08','09',\n",
    "            '10','11','12',\n",
    "            '13','14','15',\n",
    "            '16','17','18',\n",
    "            '19','20','21',\n",
    "            '22','23','24',\n",
    "            '25','26','27',\n",
    "            '28','29','30',\n",
    "            '31',\n",
    "        'area': [\n",
    "            -13.8, -71.1, -13.8,\n",
    "            -71.0,\n",
    "        ], #North, West, South, East. Default: global\n",
    "        'time' : [\n",
    "            '00:00','01:00','02:00',\n",
    "            '03:00','04:00','05:00',\n",
    "            '06:00','07:00','08:00',\n",
    "            '09:00','10:00','11:00',\n",
    "            '12:00','13:00','14:00',\n",
    "            '15:00','16:00','17:00',\n",
    "            '18:00','19:00','20:00',\n",
    "            '21:00','22:00','23:00',\n",
    "        ],\n",
    "    },\n",
    "    'download.nc') #output file name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. CMIP6 CESM2 SSP245, SSP585, historical (2015 - 2100)\n",
    "I downloaded data (r4i1p1f1, r10i1p1f1, r11i1p1f1) using wget.sh. I use the shell script below to obtain .nc files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a shell script. Copy this code and place in any txt reader.\n",
    "## It is a loop script so that I just run once.\n",
    "\n",
    "#!/bin/basH\n",
    "for i in /RAID/home/jsae-lim/CESM/*; do #location\n",
    "    echo $i\n",
    "    cd $i\n",
    "    for j in /$i/*; do #files in sub-folder\n",
    "        echo $j\n",
    "        bash $j -H\n",
    "        {Enter}\n",
    "        {Enter}\n",
    "    done\n",
    "    cd ..\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I extracted, joined data, and save data in .csv.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Quantile mapping bias correction\n",
    "The ERA5-Land reanalysis was bias corrected relative to weather station data using the quantile mapping method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the ERA5-Land reanalysis variable\n",
    "era = pd.read_csv('ERA5_1981-2019_updated_perutime.csv')  \n",
    "era['datetime'] = pd.to_datetime(era['datetime'], format='%m/%d/%Y %H:%M')  # convert to datetime format\n",
    "era['Month'] = era['datetime'].dt.month\n",
    "era.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the observed variable\n",
    "obs = pd.read_csv(\"Master_radiation_2010-2011.csv\")   #longwave radiation (strd/L_down) as an example\n",
    "obs['datetime'] = pd.to_datetime(obs['datetime']) \n",
    "obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the reanalysis and observation using inner join\n",
    "merged = pd.merge(obs[['datetime','L_down']],era[['datetime','strd']],how='inner',left_on='datetime', right_on='datetime')   \n",
    "merged = merged.dropna(axis=0, how='any')\n",
    "merged['Month'] = merged['datetime'].dt.month  \n",
    "merged = merged.reset_index()\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply quantile mapping\n",
    "mod_ecdf_w = ECDF(merged['strd_cor'].loc[(merged['Month'] <= 3) | (merged['Month'] >=10)])\n",
    "mod_ecdf_s = ECDF(merged['strd_cor'].loc[(merged['Month'] >= 4) & (merged['Month'] <=9)])\n",
    "\n",
    "for j in range(len(era.datetime)):\n",
    "    if (era.at[j,'Month'] <=3) or (era.at[j,'Month'] >= 10):\n",
    "        p = mod_ecdf_w(era.at[j,'strd_cor']) * 100   \n",
    "        corr = np.percentile(merged['L_down'].loc[(merged['Month'] <= 3) | (merged['Month'] >=10)], p) - np.percentile(\n",
    "            merged['strd_cor'].loc[(merged['Month'] <= 3) | (merged['Month'] >=10)], p)      \n",
    "    else:\n",
    "        p = mod_ecdf_s(era.at[j,'strd_cor']) * 100   \n",
    "        corr = np.percentile(merged['L_down'].loc[(merged['Month'] >= 4) & (merged['Month'] <=9)], p) - np.percentile(\n",
    "            merged['strd_cor'].loc[(merged['Month'] >= 4) & (merged['Month'] <=9)], p)\n",
    "    era.at[j,'qm_strd'] = era.at[j,'strd_cor'] + corr\n",
    "    \n",
    "era.head()\n",
    "# Print\n",
    "era.to_csv('qm_strd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Delta method\n",
    "The CMIP6 CESM2 SSP245 and SSP585 were corrected relative to the quantile mapped ERA5-Land using the delta method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files\n",
    "# We use daily quantile mapped ERA5-Land because the resolution for CESM data is daily. \n",
    "era = pd.read_csv(\"ERA_daily.csv\") #Quantile mapped ERA5-Land\n",
    "hist_10 = pd.read_csv(\"hist_10.csv\") #historical r10i1p1f1\n",
    "hist_04 = pd.read_csv(\"hist_04.csv\") #historical r4i1p1f1\n",
    "hist_11 = pd.read_csv(\"hist_11.csv\") #historical r11i1p1f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "#Group to 365 days\n",
    "era_year = era.groupby(by = [\"Month\",\"Day\"]).mean().reset_index()\n",
    "hist_10_year = hist_10.groupby(by = [\"Month\",\"Day\"]).mean().reset_index()\n",
    "hist_04_year = hist_04.groupby(by = [\"Month\",\"Day\"]).mean().reset_index()\n",
    "hist_11_year = hist_11.groupby(by = [\"Month\",\"Day\"]).mean().reset_index()\n",
    "\n",
    "#Expand to 86 years\n",
    "era_expand = pd.concat([era_year]*86).reset_index()\n",
    "hist_10_expand = pd.concat([hist_10_year]*86).reset_index()\n",
    "hist_04_expand = pd.concat([hist_04_year]*86).reset_index()\n",
    "hist_11_expand = pd.concat([hist_11_year]*86).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import SSP245 and 585 files and calculate for 'Delta'.\n",
    "# Create a list for a certain extension.\n",
    "path = r\"C:\\Users\\...\\ssp\\*.csv\" # type desired extension here.\n",
    "path_list = glob.glob(path)\n",
    "variables = [\"t2m\",\"RH\",\"wind\",\"ssrd\",\"strd\",\"sp\"]\n",
    "\n",
    "for path in path_list:\n",
    "    df = pd.read_csv(path)\n",
    "    df2 = df[:-1]\n",
    "    df2_year = df2.groupby([\"month\",\"day\"]).mean().reset_index()\n",
    "    df2_expand = pd.concat([df2_year]*86).reset_index()\n",
    "    delta = pd.DataFrame()\n",
    "    delta[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    j = 1\n",
    "    for i in variables:\n",
    "        if i == \"t2m\":\n",
    "            a = df[i]-hist_04_expand[i]\n",
    "            delta[j] = a\n",
    "            j=j+1\n",
    "        else:\n",
    "            a = df[i]/hist_04_expand[i]\n",
    "            delta[j] = a\n",
    "            j=j+1\n",
    "    delta_hourly = delta.set_index('Date').resample(\"H\").ffill()\n",
    "    delta_hourly = delta_hourly[:-1] \n",
    "    delta_hourly = delta_hourly[~((delta_hourly.index.month == 2) & (delta_hourly.index.day == 29))]\n",
    "    delta_hourly = delta_hourly.reset_index()\n",
    "    delta_hourly.to_csv(path[-13:-4]+\"_delta04.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate to get bias corrected SSP245 and 585 files.\n",
    "path = r\"C:\\Users\\...\\delta\\*.csv\" # type desired extension here.\n",
    "path_list = glob.glob(path) # then you can use codes from 1.2 to import files.\n",
    "variables = [\"t2m\",\"RH\",\"wind\",\"ssrd\",\"strd\",\"sp\"]\n",
    "\n",
    "for path in path_list:\n",
    "    df = pd.read_csv(path)\n",
    "    corr = pd.DataFrame()\n",
    "    corr[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    j = 1\n",
    "    for i in variables:\n",
    "        if i == \"t2m\":\n",
    "            a = df[str(j)]+era_hexpand[i]\n",
    "            corr[i] = a\n",
    "            j = j+1\n",
    "        else:\n",
    "            a = df[str(j)]*era_hexpand[i]\n",
    "            corr[i] = a\n",
    "            j = j+1\n",
    "    corr.to_csv(path[-20:-4]+\"_input.csv\")\n",
    "    \n",
    "# Add date/time\n",
    "path = r\"C:\\Users\\...\\input_corr\\*.csv\" # type desired extension here.\n",
    "path_list = glob.glob(path) # then you can use codes from 1.2 to import files.\n",
    "\n",
    "for path in path_list:\n",
    "    dt = pd.read_csv(\"datetime.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    dt[\"t2m\"] = df[\"t2m\"]\n",
    "    dt[\"RH\"] = df[\"RH\"]\n",
    "    dt[\"wind\"] = df[\"wind\"]\n",
    "    dt[\"ssrd\"] = df[\"ssrd\"]\n",
    "    dt[\"strd\"] = df[\"strd\"]\n",
    "    dt[\"sp\"] = df[\"sp\"]\n",
    "    dt.to_csv(path[-26:-4]+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lake model calibration\n",
    "The lake model used in this project can be found here (https://github.com/carriemorrill/lake-model). The model was tuned using the bathymetric information collected from the field during 2018 and 2019 field seasons. We performed a calibration test using 1000 ensembles of ETA and CDRN parameter choices (Latin hypercube sampling) and compared relative to the lake temperature observations from Michelutti et al. (2019). Lake model parameter files can be found at lake.inc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Latin Hypercube sampling\n",
    "params = pd.read_csv('lake-params.txt', delim_whitespace=True, header=None) \n",
    "params['cdrn'] = 2.e-3*params[0] + 1.e-3  #  neutral drag coefficient from 1 to 3 e-3\n",
    "params['eta'] = 0.3*params[1] + 0.2       # shortwave extinction coefficient from 0.2 to 0.5\n",
    "trials = range(1,1001)\n",
    "params['trial']=trials\n",
    "params[['cdrn','eta']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulations were evaluated based on NSE, Bias, and RSR. Here is an example of indices calculation based on a single lake model simulation. We run loops to get all indices for all 1000 simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lake temperature observation (South end temperatures)\n",
    "obs = pd.read_csv(r'C:\\Users\\...\\SIB_south_2016-2018.csv',engine='python')  #, delim_whitespace=True) \n",
    "obs['datetime'] = pd.to_datetime(obs['Date'], format='%m/%d/%Y')  # convert to datetime format\n",
    "obs['Month'] = obs['datetime'].dt.month      # create Month column from datetime\n",
    "obs['Year'] = obs['datetime'].dt.year    # create Year column from datetime\n",
    "obs['Day'] = obs['datetime'].dt.day\n",
    "obs_day =obs.groupby(['Year','Month','Day']).mean()  # groupby Year, Month, Day and take average of groups --> daily averages\n",
    "obs_day1D = np.ravel(obs_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import lake temperature simulations. It has to have the same shape as the observation.\n",
    "model = pd.read_csv(r'C:\\Users\\...\\profile.dat', delim_whitespace=True, header=None,engine='python',\n",
    "     skiprows=13025, skipfooter=507, usecols=(3,6,9,12,15,18,21,24,27,30)) \n",
    "model.tail()\n",
    "model_1D = np.ravel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate values for objective functions to choose best calibration simulations\n",
    "def bias(predictions, targets): \n",
    "    return (targets.mean() - predictions.mean())\n",
    "def NashSut(predictions, targets):\n",
    "    return (1. - ((predictions - targets) ** 2).sum()/((targets - targets.mean()) ** 2).sum())\n",
    "def rsr(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).sum())/np.sqrt(((targets - targets.mean()) ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nash Sutcliffe efficiency\n",
    "NashSut(model_1D,obs_day1D)                 # all depths\n",
    "#NashSut(model_1D[0:351],obs_day1D[0:351])  # LST\n",
    "\n",
    "#Bias (Observations - Model)\n",
    "bias(model_1D,obs_day1D)                  # all depths\n",
    "#bias(model_1D[0:351],obs_day1D[0:351])   # LST\n",
    "\n",
    "#RSR (Root mean square error standard deviation ratio)\n",
    "rsr(model_1D,obs_day1D)  # want <= 0.5    # all depths\n",
    "#rsr(model_1D[0:351],obs_day1D[0:351])    # just lake surface temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Reanalysis, raw ERA5-Land, and quantile mapped ERA5-Land comparison\n",
    "Example: Figure 2a Solar radiation downward (hourly average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "df = pd.read_csv(r'Z:\\Lake_model\\final\\Figures\\data\\radiation_month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6), dpi = 300)\n",
    "\n",
    "#Raw ERA5-Land\n",
    "ax.plot(df.hour, df.ssrd, linewidth=1,color= \"#ff0066\", alpha = 1)\n",
    "ax.fill_between(df.hour, df.ssrd+df.ssrd_std, df.ssrd-df.ssrd_std, facecolor='#ff0066', alpha=0.1)\n",
    "\n",
    "#Quantile mapped ERA5-Land\n",
    "ax.plot(df.hour, df.ssrd_qm, linewidth=2,color= \"#000000\", alpha = 1)\n",
    "ax.fill_between(df.hour, df.ssrd_qm+df.ssrd_qm_std, df.ssrd_qm-df.ssrd_qm_std, facecolor='#000000', alpha=0.1)\n",
    "\n",
    "#Observation\n",
    "ax.plot(df.hour, df.k_down, linewidth=0.8,color=\"#0000ff\", alpha = 1, marker = \"o\", markersize = 3)\n",
    "ax.errorbar(x = df.hour, y = df.k_down, yerr = df.k_down_std, ecolor='#0000ff', capsize=4, linewidth= 0.8,\n",
    "           markeredgewidth=0.5, alpha = 0.5) \n",
    "\n",
    "ax.set_ylim(0,1200)# y-axis\n",
    "ax.set_xlim(0,23) # x-axis (hour in a day)\n",
    "ax.set_xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23])\n",
    "\n",
    "fig.savefig('Figure2a.jpg', dpi = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Lake temperature-depth profile visualization\n",
    "Figure 4a - 4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.dates import MonthLocator, DateFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom colormap\n",
    "hexcode = ['#020024', '#02002c', '#030134', '#03023d', '#040245', '#04034d', '#050456', '#05055e', '#060566', '#06066f', '#070777', '#080880', '#071485', '#06208b', '#052c90', '#053896', '#04449b', '#0350a1', '#025ca6', '#0268ac', '#0174b1', '#0080b7', '#008dbd', '#0092b7', '#0097b1', '#009dab', '#00a2a5', '#00a79f', '#00ad99', '#00b293', '#00b78d', '#00bd87', '#00c281', '#00c87c', '#17cc76', '#2ed170', '#45d56a', '#5cda64', '#73df5e', '#8be358', '#a2e852', '#b9ed4c', '#d0f146', '#e7f640', '#fffb3b']\n",
    "custom = colors.ListedColormap(hexcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lake temperature observations and simulation\n",
    "# South end temperature\n",
    "obs = pd.read_csv(r'C:\\...\\south.csv',engine='python')  #, delim_whitespace=True) \n",
    "obs['datetime'] = pd.to_datetime(obs['Date'], format='%m/%d/%Y')  # convert to datetime format\n",
    "obs['Month'] = obs['datetime'].dt.month      # create Month column from datetime\n",
    "obs['Year'] = obs['datetime'].dt.year    # create Year column from datetime\n",
    "obs['Day'] = obs['datetime'].dt.day\n",
    "obs_day =obs.groupby(['Year','Month','Day']).mean()  # groupby Year, Month, Day and take average of groups --> daily averages\n",
    "obs_day1D = np.ravel(obs_day)\n",
    "\n",
    "# North end temperature\n",
    "obs2 = pd.read_csv(r'C:\\...\\north.csv',engine='python')  #, delim_whitespace=True) \n",
    "obs2['datetime'] = pd.to_datetime(obs2['Date'], format='%m/%d/%Y')  # convert to datetime format\n",
    "obs2['Month'] = obs2['datetime'].dt.month      # create Month column from datetime\n",
    "obs2['Year'] = obs2['datetime'].dt.year    # create Year column from datetime\n",
    "obs2['Day'] = obs2['datetime'].dt.day\n",
    "obs2_day =obs2.groupby(['Year','Month','Day']).mean()  # groupby Year, Month, Day and take average of groups --> daily averages\n",
    "obs2_day1D = np.ravel(obs2_day)\n",
    "\n",
    "# Simulated temperature\n",
    "model = pd.read_csv(r'C:\\...\\profile.dat', delim_whitespace=True, header=None,engine='python',\n",
    "model_1D = np.ravel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "levels = [6,6.5,7,7.5,8,8.5,9,9.5,10,10.5,11,11.5,12]\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "month_fmt = DateFormatter('%b')\n",
    "def m_fmt(x, pos=None):\n",
    "    return month_fmt(x)[0]\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(6, 12), dpi = 300)\n",
    "\n",
    "# Figure 4a\n",
    "x1 = np.arange(np.datetime64('2016-08-30'), np.datetime64('2018-08-12'))\n",
    "y1 = np.array([0, -3, -6, -9, -12, -15, -18, -21, -24, -27])\n",
    "X1, Y1 = np.meshgrid(x1, y1)\n",
    "\n",
    "CS1 = ax1.contourf(X1, Y1, obs2_day[[\"0m\",\"3m\",\"6m\",\"9m\",\"12m\",\"15m\",\"18m\",\"21m\",\"24m\",\"27m\"]].T,levels,cmap=custom)\n",
    "#ax.clabel(CS1, inline=1, fontsize=10)\n",
    "ax1.set_title('Observed Sibinacocha lake temperature (north end)')\n",
    "\n",
    "ax1.xaxis.set_major_locator(MonthLocator())\n",
    "ax1.xaxis.set_major_formatter(FuncFormatter(m_fmt))\n",
    "ax1.set_ylabel('Lake depth (m)')\n",
    "\n",
    "# Figure 4b\n",
    "x2 = np.arange(np.datetime64('2016-08-30'), np.datetime64('2018-08-12'))\n",
    "y2 = np.array([0, -2,-5,-7,-10,-12,-15,-17,-19,-22, -27])\n",
    "X2, Y2 = np.meshgrid(x2, y2)\n",
    "\n",
    "CS2 = ax2.contourf(X2, Y2, obs_day[[\"0m\",\"2.4m\",\"4.9m\",\"7.3m\",\"9.8m\",\"12.2m\",\"14.6m\",\"17.1m\",\"19.5m\",\"21.9m\",\"27m\"]].T,levels,cmap=custom)\n",
    "#ax.clabel(CS1, inline=1, fontsize=10)\n",
    "ax2.set_title('Observed Sibinacocha lake temperature (south end)')\n",
    "\n",
    "ax2.xaxis.set_major_locator(MonthLocator())\n",
    "ax2.xaxis.set_major_formatter(FuncFormatter(m_fmt))\n",
    "ax2.set_ylabel('Lake depth (m)')\n",
    "\n",
    "# Figure 4c\n",
    "x3 = np.arange(np.datetime64('2016-08-30'), np.datetime64('2018-08-12'))\n",
    "y3 = np.array([0, -3, -6, -9, -12, -15, -18, -21, -24, -27])\n",
    "X3, Y3 = np.meshgrid(x3, y3)\n",
    "\n",
    "CS3 = ax3.contourf(X3, Y3, model.T,levels, cmap=custom)\n",
    "#ax.clabel(CS1, inline=1, fontsize=10)\n",
    "ax3.set_title('Modeled Sibinacocha lake temperature (top 27 m)')\n",
    "\n",
    "ax3.xaxis.set_major_locator(MonthLocator())\n",
    "ax3.xaxis.set_major_formatter(FuncFormatter(m_fmt))\n",
    "ax3.set_ylabel('Lake depth (m)')\n",
    "\n",
    "#color bar\n",
    "fig.subplots_adjust(bottom=0.1)\n",
    "cbar_ax = fig.add_axes([0.1, 0.05, 0.8, 0.015,])\n",
    "fig.colorbar(CS1, cax = cbar_ax, orientation='horizontal', label='Lake temperature (℃)' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
